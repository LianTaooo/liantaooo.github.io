<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Intermediate2-dnn]]></title>
    <url>%2F2018%2F06%2F22%2FIntermediate2-dnn%2F</url>
    <content type="text"><![CDATA[残差网络resource: https://arxiv.org/pdf/1512.03385.pdfcode in the below! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166import torchimport torch.nn as nnimport torchvisionimport torchvision.transforms as transforms# Device configurationdevice = torch.device('cuda：0' if torch.cuda.is_available() else 'cpu')# Hyper-parametersnum_epochs = 80learning_rate = 0.001# Image preprocessing modulestransform = transforms.Compose([ transforms.Pad(4), transforms.RandomHorizontalFlip(), transforms.RandomCrop(32), transforms.ToTensor()])# CIFAR-10 datasettrain_dataset = torchvision.datasets.CIFAR10(root='./data/mnist/', train=True, transform=transform, download=True)test_dataset = torchvision.datasets.CIFAR10(root='./data/mnist/', train=False, transform=transforms.ToTensor())# Data loadertrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)# 3x3 convolutiondef conv3x3(in_channels, out_channels, stride=1): return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)# Residual blockclass ResidualBlock(nn.Module): def __init__(self, in_channels, out_channels, stride=1, downsample=None): super(ResidualBlock, self).__init__() self.conv1 = conv3x3(in_channels, out_channels, stride) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(out_channels, out_channels) self.bn2 = nn.BatchNorm2d(out_channels) self.downsample = downsample def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample: residual = self.downsample(x) out += residual out = self.relu(out) return out# ResNetclass ResNet(nn.Module): def __init__(self, block, layers, num_classes=10): super(ResNet, self).__init__() self.in_channels = 16 self.conv = conv3x3(3, 16) self.bn = nn.BatchNorm2d(16) self.relu = nn.ReLU(inplace=True) self.layer1 = self.make_layer(block, 16, layers[0]) self.layer2 = self.make_layer(block, 32, layers[0], 2) self.layer3 = self.make_layer(block, 64, layers[1], 2) self.avg_pool = nn.AvgPool2d(8) self.fc = nn.Linear(64, num_classes) def make_layer(self, block, out_channels, blocks, stride=1): downsample = None if (stride != 1) or (self.in_channels != out_channels): downsample = nn.Sequential( conv3x3(self.in_channels, out_channels, stride=stride), nn.BatchNorm2d(out_channels)) layers = [] layers.append(block(self.in_channels, out_channels, stride, downsample)) self.in_channels = out_channels for i in range(1, blocks): layers.append(block(out_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): out = self.conv(x) out = self.bn(out) out = self.relu(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.avg_pool(out) out = out.view(out.size(0), -1) out = self.fc(out) return outmodel = ResNet(ResidualBlock, [2, 2, 2, 2]).to(device)# Loss and optimizercriterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)# For updating learning ratedef update_lr(optimizer, lr): for param_group in optimizer.param_groups: param_group['lr'] = lr# Train the modeltotal_step = len(train_loader)curr_lr = learning_ratefor epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): images = images.to(device) labels = labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i + 1) % 100 == 0: print("Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;] Loss: &#123;:.4f&#125;" .format(epoch + 1, num_epochs, i + 1, total_step, loss.item())) # Decay learning rate if (epoch + 1) % 20 == 0: curr_lr /= 3 update_lr(optimizer, curr_lr)# Test the modelmodel.eval()with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the model on the test images: &#123;&#125; %'.format(100 * correct / total))# Save the model checkpointtorch.save(model.state_dict(), 'resnet.ckpt')]]></content>
  </entry>
  <entry>
    <title><![CDATA[Intermediate1-cnn]]></title>
    <url>%2F2018%2F06%2F22%2FIntermediate1-cnn%2F</url>
    <content type="text"><![CDATA[MNIST两个卷积, 一个全连接;test error &lt; 1% 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import torchimport torch.nn as nnimport torchvisionimport torchvision.transforms as transforms# Device configurationdevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')# Hyper parametersnum_epochs = 5num_classes = 10batch_size = 100learning_rate = 0.001# MNIST datasettrain_dataset = torchvision.datasets.MNIST(root='./data/mnist/', train=True, transform=transforms.ToTensor(), download=True)test_dataset = torchvision.datasets.MNIST(root='./data//mnist/', train=False, transform=transforms.ToTensor())# Data loadertrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)# Convolutional neural network (two convolutional layers)class ConvNet(nn.Module): def __init__(self, num_classes=10): super(ConvNet, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2)) self.layer2 = nn.Sequential( nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2)) self.fc = nn.Linear(7 * 7 * 32, num_classes) def forward(self, x): out = self.layer1(x) out = self.layer2(out) out = out.reshape(out.size(0), -1) #行为样本数 out = self.fc(out) return outmodel = ConvNet(num_classes).to(device)# Loss and optimizercriterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)# Train the modeltotal_step = len(train_loader)for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): images = images.to(device) labels = labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i + 1) % 100 == 0: print('Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;' .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))# Test the modelmodel.eval() # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Test Accuracy of the model on the 10000 test images: &#123;&#125; %'.format(100 * correct / total))# Save the model checkpointtorch.save(model.state_dict(), 'model.ckpt')]]></content>
  </entry>
  <entry>
    <title><![CDATA[pytorch-basics 4(feedforward neural network)]]></title>
    <url>%2F2018%2F06%2F22%2Fpytorch-basics4%2F</url>
    <content type="text"><![CDATA[三层网络MNIST手写体识别 (test error:2.41%) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import torchimport torch.nn as nnimport torchvisionimport torchvision.transforms as transforms# Device configurationdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')# Hyper-parametersinput_size = 784hidden_size = 500num_classes = 10num_epochs = 5batch_size = 100learning_rate = 0.001# MNIST datasettrain_dataset = torchvision.datasets.MNIST(root='./data/mnist/', train=True, transform=transforms.ToTensor(), download=True)test_dataset = torchvision.datasets.MNIST(root='./data/mnist/', train=False, transform=transforms.ToTensor())# Data loadertrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)# Fully connected neural network with one hidden layerclass NeuralNet(nn.Module): def __init__(self, input_size, hidden_size, num_classes): super(NeuralNet, self).__init__() #NeuralNet 父类 Module self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, num_classes) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return outmodel = NeuralNet(input_size, hidden_size, num_classes).to(device)# Loss and optimizercriterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)# Train the modeltotal_step = len(train_loader)for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # Move tensors to the configured device images = images.reshape(-1, 28 * 28).to(device) labels = labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i + 1) % 100 == 0: print('Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;' .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))# Test the model# In test phase, we don't need to compute gradients (for memory efficiency)with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.reshape(-1, 28 * 28).to(device) labels = labels.to(device) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: &#123;&#125; %'.format(100 * correct / total))# Save the model checkpointtorch.save(model.state_dict(), 'model.ckpt')]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-basics 3]]></title>
    <url>%2F2018%2F06%2F22%2Fpytorch-basics3%2F</url>
    <content type="text"><![CDATA[逻辑回归，用neural network 框架做 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import torchimport torch.nn as nnimport torchvisionimport torchvision.transforms as transforms# Hyper-parametersinput_size = 784num_classes = 10num_epochs = 5batch_size = 100learning_rate = 0.001# MNIST dataset (images and labels)train_dataset = torchvision.datasets.MNIST(root='./data/mnist/', train=True, transform=transforms.ToTensor(), download=True)test_dataset = torchvision.datasets.MNIST(root='./data/mnist/', train=False, transform=transforms.ToTensor())# Data loader (input pipeline)train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)# Logistic regression modelmodel = nn.Linear(input_size, num_classes)# Loss and optimizer# nn.CrossEntropyLoss() computes softmax internallycriterion = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)# Train the modeltotal_step = len(train_loader) #一个epoch内迭代的次数for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # Reshape images to (batch_size, input_size) images = images.reshape(-1, 28 * 28) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i + 1) % 100 == 0: print('Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;' .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))# Test the model# In test phase, we don't need to compute gradients (for memory efficiency)with torch.no_grad(): correct = 0 total = 0 for images, labels in test_loader: images = images.reshape(-1, 28 * 28) outputs = model(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum() print('Accuracy of the model on the 10000 test images: &#123;&#125; %'.format(100 * correct / total))# Save the model checkpointtorch.save(model.state_dict(), 'model.ckpt')]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-basics 2]]></title>
    <url>%2F2018%2F06%2F22%2Fpytorch-basics2%2F</url>
    <content type="text"><![CDATA[一元线性回归，用neural network 框架做y = w*x + b 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import torchimport torch.nn as nnimport numpy as npimport matplotlib.pyplot as plt# Hyper-parametersinput_size = 1output_size = 1num_epochs = 60learning_rate = 0.001# Toy datasetx_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], [9.779], [6.182], [7.59], [2.167], [7.042], [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], [3.366], [2.596], [2.53], [1.221], [2.827], [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)# Linear regression modelmodel = nn.Linear(input_size, output_size)# Loss and optimizercriterion = nn.MSELoss()optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)# Train the modelfor epoch in range(num_epochs): # Convert numpy arrays to torch tensors inputs = torch.from_numpy(x_train) targets = torch.from_numpy(y_train) # Forward pass outputs = model(inputs) loss = criterion(outputs, targets) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (epoch + 1) % 5 == 0: print('Epoch [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'.format(epoch + 1, num_epochs, loss.item()))# Plot the graphpredicted = model(torch.from_numpy(x_train)).detach().numpy() #模型不需要去算梯度plt.plot(x_train, y_train, 'ro', label='Original data')plt.plot(x_train, predicted, label='Fitted line') #不需要取出参数再去画直线。直接带入模型，结果都在直线上plt.legend()plt.show()# Save the model checkpointtorch.save(model.state_dict(), 'model.ckpt')]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch_basics 1]]></title>
    <url>%2F2018%2F06%2F21%2Fpytorch-basics1%2F</url>
    <content type="text"><![CDATA[基本使用123456789101112131415161718import torch import torchvisionimport torch.nn as nnimport numpy as npimport torchvision.transforms as transforms# ================================================================== ## Table of Contents ## ================================================================== ## 1. Basic autograd example 1 # 2. Basic autograd example 2 # 3. Loading data from numpy # 4. Input pipline # 5. Input pipline for custom dataset # 6. Pretrained model # 7. Save and load model 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169# ================================================================== ## 1. Basic autograd example 1 ## ================================================================== ## Create tensors.x = torch.tensor(1, requires_grad=True)w = torch.tensor(2, requires_grad=True)b = torch.tensor(3, requires_grad=True)# Build a computational graph.y = w * x + b # y = 2 * x + 3# Compute gradients.y.backward()# Print out the gradients.print(x.grad) # x.grad = 2 print(w.grad) # w.grad = 1 print(b.grad) # b.grad = 1 # ================================================================== ## 2. Basic autograd example 2 ## ================================================================== ## Create tensors of shape (10, 3) and (10, 2).x = torch.randn(10, 3)y = torch.randn(10, 2)# Build a fully connected layer.linear = nn.Linear(3, 2)print ('w: ', linear.weight)print ('b: ', linear.bias)# Build loss function and optimizer.criterion = nn.MSELoss()optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)# Forward pass.pred = linear(x)# Compute loss.loss = criterion(pred, y)print('loss: ', loss.item())# Backward pass.loss.backward()# Print out the gradients.print ('dL/dw: ', linear.weight.grad) print ('dL/db: ', linear.bias.grad)# 1-step gradient descent.optimizer.step()# You can also perform gradient descent at the low level.# linear.weight.data.sub_(0.01 * linear.weight.grad.data)# linear.bias.data.sub_(0.01 * linear.bias.grad.data)# Print out the loss after 1-step gradient descent.pred = linear(x)loss = criterion(pred, y)print('loss after 1 step optimization: ', loss.item())# ================================================================== ## 3. Loading data from numpy ## ================================================================== ## Create a numpy array.x = np.array([[1, 2], [3, 4]])# Convert the numpy array to a torch tensor.y = torch.from_numpy(x)# Convert the torch tensor to a numpy array.z = y.numpy()# ================================================================== ## 4. Input pipline ## ================================================================== ## Download and construct CIFAR-10 dataset.train_dataset = torchvision.datasets.CIFAR10(root='../../data/', train=True, transform=transforms.ToTensor(), download=True)# Fetch one data pair (read data from disk).image, label = train_dataset[0]print (image.size())print (label)# Data loader (this provides queues and threads in a very simple way).train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)# When iteration starts, queue and thread start to load data from files.data_iter = iter(train_loader)# Mini-batch images and labels.images, labels = data_iter.next()# Actual usage of the data loader is as below.for images, labels in train_loader: # Training code should be written here. pass# ================================================================== ## 5. Input pipline for custom dataset ## ================================================================== ## You should your build your custom dataset as below.class CustomDataset(torch.utils.data.Dataset): def __init__(self): # TODO # 1. Initialize file paths or a list of file names. pass def __getitem__(self, index): # TODO # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open). # 2. Preprocess the data (e.g. torchvision.Transform). # 3. Return a data pair (e.g. image and label). pass def __len__(self): # You should change 0 to the total size of your dataset. return 0 # You can then use the prebuilt data loader. custom_dataset = CustomDataset()train_loader = torch.utils.data.DataLoader(dataset=custom_dataset, batch_size=64, shuffle=True)# ================================================================== ## 6. Pretrained model ## ================================================================== ## Download and load the pretrained ResNet-18.resnet = torchvision.models.resnet18(pretrained=True)# If you want to finetune only the top layer of the model, set as below.for param in resnet.parameters(): param.requires_grad = False# Replace the top layer for finetuning.resnet.fc = nn.Linear(resnet.fc.in_features, 100) # 100 is an example.# Forward pass.images = torch.randn(64, 3, 224, 224)outputs = resnet(images)print (outputs.size()) # (64, 100)# ================================================================== ## 7. Save and load the model ## ================================================================== ## Save and load the entire model.torch.save(resnet, 'model.ckpt')model = torch.load('model.ckpt')# Save and load only the model parameters (recommended).torch.save(resnet.state_dict(), 'params.ckpt')resnet.load_state_dict(torch.load('params.ckpt'))]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch-example1-cifar-10]]></title>
    <url>%2F2018%2F06%2F21%2Fpytorch-example1-cifar%2F</url>
    <content type="text"><![CDATA[Classical CNN in PyTorch12345678import torchimport torchvisionimport torchvision.transforms as transformsimport matplotlib.pyplot as pltimport numpy as npimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optim 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394if __name__ == '__main__': # win 下，linux 下可以删去 #1.导入数据 transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # mean std trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') #显示图像 def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) #2.创建网络 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() #gpu 上运算 device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") net.to(device) # 3.loss function criterion = nn.CrossEntropyLoss() optimizer=optim.SGD(net.parameters(),lr=0.001,momentum=0.9) # 4.训练网络 for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 # i 范围：1~12500；trainloader 中已经把batch分好了 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data #to GPU inputs, labels = inputs.to(device), labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches batch size=4 print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training') #5. 测试集 正确率 correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data # to GPU images, labels = images.to(device), labels.to(device) outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total))]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Framework]]></title>
    <url>%2F2018%2F06%2F01%2FDeep-Learning-Framework%2F</url>
    <content type="text"><![CDATA[Framework choice?]]></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[code example]]></title>
    <url>%2F2018%2F04%2F13%2Fcode-example%2F</url>
    <content type="text"><![CDATA[MATLAB 画出的图形输出为矢量图12345678910111213141516171819202122232425262728293031% 创建figure对象Fig = figure(... 'Units', 'pixels',... 'Name', 'move2',... 'NumberTitle', 'off',... 'IntegerHandle', 'off');% 创建axes对象, 设定坐标轴属性AxesH = axes(... 'Parent', Fig,... 'Xlim', [-10 50],... 'Ylim', [-10 40],... 'XGrid', 'on',... 'YGrid', 'on',... 'DataAspectRatio', [1 1 1],... 'Visible', 'on'); t = linspace(0,45,101)*pi/180;x = 40*cos(t);y = 40*sin(t);realtrace = line(AxesH, x, y,'linewidth',2);precisetrace = line(AxesH,[x(1) x(end)] ,[y(1) y(end)],... 'color','g',... 'linewidth',2,... 'linestyle','--');legend(AxesH,[realtrace,precisetrace],... '真实运动轨迹','精确运动轨迹',... 'location','northwest');% 指定保存路径和格式saveas(Fig,['D:\abc\' Fig.Name],'pdf')]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F04%2F12%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[EnglishPod-24]]></title>
    <url>%2F2018%2F04%2F12%2FEnglishPod-24%2F</url>
    <content type="text"><![CDATA[do you two have any plans this evening** for the we were thinking checking out restaurant of the neighborhood ,do you have any suggestions of a in i know this really , i reconment you have a try nice ltalian place, the food is fantastic,and the decor is beatiful. ‘d recomend giving it a try actually, i am not cracy about ltaliy food .i want some food light. all that ltalian ‘m in the mood for something a bit lighterthe fish is outstanding. in that case, i know a great little bistro. they make a really tasty seafood platter it is sound fantastic .but i am allergic to seafood so… sounds ok, well let me think . i know hall in the wall .you could give them a try. this great little place./ it is just a hole / but they do the most amazing sandwishes / gotta allen,you took me there last time.i got food poisoning remember. ella/i visited /and you have got to=you gotta decor=decoration yummy=tasty=delicious]]></content>
      <tags>
        <tag>English Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNN-Reading list]]></title>
    <url>%2F2017%2F10%2F10%2FDNN-Reading-list%2F</url>
    <content type="text"><![CDATA[Reading-ListReading list on deep learning. Registration Non-rigid image registration using fully convolutional networks with deep self-supervision Robust non-rigid registration through agent-based action learning: Krebs, Julian, et al. “Robust non-rigid registration through agent-based action learning.” Medical Image Computing and Computer Assisted Interventions (MICCAI). springer, 2017. SVF_Net: Rohé, Marc-Michel, et al. “SVF-Net: Learning Deformable Image Registration Using Shape Matching.” MICCAI 2017-the 20th International Conference on Medical Image Computing and Computer Assisted Intervention. 2017. Basic Network and Techniques AlexNet: MLA Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012. :star::star::star::star::star: Dropout: Srivastava, Nitish, et al. “Dropout: a simple way to prevent neural networks from overfitting.” Journal of Machine Learning Research 15.1 (2014): 1929-1958. :star::star::star::star: VGG: Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014). :star::star::star::star::star: GoogLeNet: Szegedy, Christian, et al. “Going deeper with convolutions.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. :star::star::star::star::star: Batch Normalization: Ioffe, Sergey, and Christian Szegedy. “Batch normalization: Accelerating deep network training by reducing internal covariate shift.” arXiv preprint arXiv:1502.03167 (2015). [Inception v2] :star::star::star::star::star: PReLU &amp; msra Initilization: He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.” Proceedings of the IEEE international conference on computer vision. 2015. :star::star::star::star::star: InceptionV3: Szegedy, Christian, et al. “Rethinking the inception architecture for computer vision.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star::star: ResNet: He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star::star::star: Identity ResNet: He, Kaiming, et al. “Identity mappings in deep residual networks.” European Conference on Computer Vision. Springer International Publishing, 2016. :star::star::star::star::star: CReLU: Shang, Wenling, et al. “Understanding and improving convolutional neural networks via concatenated rectified linear units.” Proceedings of the International Conference on Machine Learning (ICML). 2016. :star::star::star: InceptionV4 &amp; Inception-ResNet: Szegedy, Christian, et al. “Inception-v4, inception-resnet and the impact of residual connections on learning.” arXiv preprint arXiv:1602.07261 (2016). :star::star::star::star: ResNeXt: Xie, Saining, et al. “Aggregated residual transformations for deep neural networks.” arXiv preprint arXiv:1611.05431 (2016). :star::star::star::star: Batch Renormalization: Ioffe, Sergey. “Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models.” arXiv preprint arXiv:1702.03275 (2017). :star::star::star::star: Xception: Chollet, François. “Xception: Deep Learning with Depthwise Separable Convolutions.” arXiv preprint arXiv:1610.02357 (2016). :star::star::star: MobileNets: Howard, Andrew G., et al. “Mobilenets: Efficient convolutional neural networks for mobile vision applications.” arXiv preprint arXiv:1704.04861 (2017). :star::star::star: DenseNet: Huang, Gao, et al. “Densely connected convolutional networks.” arXiv preprint arXiv:1608.06993 (2016). :star::star::star::star::star: PolyNet: Zhang, Xingcheng, et al. “Polynet: A pursuit of structural diversity in very deep networks.” arXiv preprint arXiv:1611.05725 (2016). Slides :star::star::star::star: Object Detection Overfeat: Sermanet, Pierre, et al. “Overfeat: Integrated recognition, localization and detection using convolutional networks.” arXiv preprint arXiv:1312.6229 (2013). :star::star::star::star: RCNN: Girshick, Ross, et al. “Rich feature hierarchies for accurate object detection and semantic segmentation.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. :star::star::star::star::star: SPP: He, Kaiming, et al. “Spatial pyramid pooling in deep convolutional networks for visual recognition.” European Conference on Computer Vision. Springer International Publishing, 2014. :star::star::star::star::star: Fast RCNN: Girshick, Ross. “Fast r-cnn.” Proceedings of the IEEE International Conference on Computer Vision. 2015. :star::star::star::star::star: Faster RCNN: Ren, Shaoqing, et al. “Faster r-cnn: Towards real-time object detection with region proposal networks.” Advances in neural information processing systems. 2015. :star::star::star::star::star: R-CNN minus R: Lenc, Karel, and Andrea Vedaldi. “R-cnn minus r.” arXiv preprint arXiv:1506.06981 (2015). :star: End-to-end people detection in crowded scenes: Stewart, Russell, Mykhaylo Andriluka, and Andrew Y. Ng. “End-to-end people detection in crowded scenes.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star: YOLO: Redmon, Joseph, et al. “You only look once: Unified, real-time object detection.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star::star::star: ION: Bell, Sean, et al. “Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star::star: MultiPath: Zagoruyko, Sergey, et al. “A multipath network for object detection.” arXiv preprint arXiv:1604.02135 (2016). :star::star::star: SSD: Liu, Wei, et al. “SSD: Single shot multibox detector.” European Conference on Computer Vision. Springer International Publishing, 2016. :star::star::star::star::star: OHEM: Shrivastava, Abhinav, Abhinav Gupta, and Ross Girshick. “Training region-based object detectors with online hard example mining.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star::star::star: HyperNet: Kong, Tao, et al. “HyperNet: towards accurate region proposal generation and joint object detection.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star::star: SDP: Yang, Fan, Wongun Choi, and Yuanqing Lin. “Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star::star: SubCNN: Xiang, Yu, et al. “Subcategory-aware convolutional neural networks for object proposals and detection.” Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on. IEEE, 2017. :star::star::star: MSCNN: Cai, Zhaowei, et al. “A unified multi-scale deep convolutional neural network for fast object detection.” European Conference on Computer Vision. Springer International Publishing, 2016. :star::star::star::star: RFCN: Li, Yi, Kaiming He, and Jian Sun. “R-fcn: Object detection via region-based fully convolutional networks.” Advances in Neural Information Processing Systems. 2016. :star::star::star::star::star: Shallow Network: Ashraf, Khalid, et al. “Shallow networks for high-accuracy road object-detection.” arXiv preprint arXiv:1606.01561 (2016). :star::star: Is Faster R-CNN Doing Well for Pedestrian Detection: Zhang, Liliang, et al. “Is Faster R-CNN Doing Well for Pedestrian Detection?.” European Conference on Computer Vision. Springer International Publishing, 2016. :star::star: GCNN: Najibi, Mahyar, Mohammad Rastegari, and Larry S. Davis. “G-cnn: an iterative grid based object detector.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star: LocNet: Gidaris, Spyros, and Nikos Komodakis. “Locnet: Improving localization accuracy for object detection.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star: PVANet: Kim, Kye-Hyeon, et al. “PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection.” arXiv preprint arXiv:1608.08021 (2016). :star::star::star::star: FPN: Lin, Tsung-Yi, et al. “Feature Pyramid Networks for Object Detection.” arXiv preprint arXiv:1612.03144 (2016). :star::star::star::star::star: TDM: Shrivastava, Abhinav, et al. “Beyond Skip Connections: Top-Down Modulation for Object Detection.” arXiv preprint arXiv:1612.06851 (2016). :star::star::star::star: YOLO9000: Redmon, Joseph, and Ali Farhadi. “YOLO9000: Better, Faster, Stronger.” arXiv preprint arXiv:1612.08242 (2016). :star::star::star::star: Speed/accuracy trade-offs for modern convolutional object detectors: Huang, Jonathan, et al. “Speed/accuracy trade-offs for modern convolutional object detectors.” arXiv preprint arXiv:1611.10012 (2016). :star::star: GDB-Net: Zeng, Xingyu, et al. “Crafting GBD-Net for Object Detection.” arXiv preprint arXiv:1610.02579 (2016). Slides :star::star::star::star: WRInception: Lee, Youngwan, et al. “Wide-Residual-Inception Networks for Real-time Object Detection.” arXiv preprint arXiv:1702.01243 (2017). :star: DSSD: Fu, Cheng-Yang, et al. “DSSD: Deconvolutional Single Shot Detector.” arXiv preprint arXiv:1701.06659 (2017). :star::star::star::star: A-Fast-RCNN (Hard positive generation): Wang, Xiaolong, Abhinav Shrivastava, and Abhinav Gupta. “A-fast-rcnn: Hard positive generation via adversary for object detection.” arXiv preprint arXiv:1704.03414 (2017). :star::star::star: RRC: Ren, Jimmy, et al. “Accurate Single Stage Detector Using Recurrent Rolling Convolution.” arXiv preprint arXiv:1704.05776 (2017). :star::star::star: Deformable ConvNets: Dai, Jifeng, et al. “Deformable Convolutional Networks.” arXiv preprint arXiv:1703.06211 (2017). :star::star::star::star: RSSD: Jeong, Jisoo, Hyojin Park, and Nojun Kwak. “Enhancement of SSD by concatenating feature maps for object detection.” arXiv preprint arXiv:1705.09587 (2017). :star::star: Perceptual GAN: Li, Jianan, et al. “Perceptual Generative Adversarial Networks for Small Object Detection.” arXiv preprint arXiv:1706.05274 (2017). :star::star::star: Semantic Segmentation FCN: Long, Jonathan, Evan Shelhamer, and Trevor Darrell. “Fully convolutional networks for semantic segmentation.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. :star::star::star::star::star: Deconvolution Network for Segmentation: Noh, Hyeonwoo, Seunghoon Hong, and Bohyung Han. “Learning deconvolution network for semantic segmentation.” Proceedings of the IEEE International Conference on Computer Vision. 2015. :star::star::star: MNC: Dai, Jifeng, Kaiming He, and Jian Sun. “Instance-aware semantic segmentation via multi-task network cascades.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star::star::star: InstanceFCN: Dai, Jifeng, et al. “Instance-sensitive fully convolutional networks.” arXiv preprint arXiv:1603.08678 (2016). :star::star::star::star: FCIS: Li, Yi, et al. “Fully convolutional instance-aware semantic segmentation.” arXiv preprint arXiv:1611.07709 (2016). :star::star::star::star::star: Mask R-CNN: He, Kaiming, et al. “Mask r-cnn.” arXiv preprint arXiv:1703.06870 (2017). :star::star::star::star::star: Weakly Supervised Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning: Cinbis, Ramazan Gokberk, Jakob Verbeek, and Cordelia Schmid. “Weakly supervised object localization with multi-fold multiple instance learning.” IEEE transactions on pattern analysis and machine intelligence 39.1 (2017): 189-203. :star::star::star: Weakly Supervised Deep Detection Networks: Bilen, Hakan, and Andrea Vedaldi. “Weakly supervised deep detection networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. :star::star::star::star: Weakly- and Semi-Supervised Learning: Papandreou, George, et al. “Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation.” Proceedings of the IEEE International Conference on Computer Vision. 2015. :star::star::star::star: Image-level to pixel-level labeling: Pinheiro, Pedro O., and Ronan Collobert. “From image-level to pixel-level labeling with convolutional networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. Weakly Supervised Localization using Deep Feature Maps: Bency, Archith J., et al. “Weakly supervised localization using deep feature maps.” arXiv preprint arXiv:1603.00489 (2016). WELDON: Durand, Thibaut, Nicolas Thome, and Matthieu Cord. “Weldon: Weakly supervised learning of deep convolutional neural networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. WILDCAT: Durand, Thibaut, et al. “WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation.” The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017. SGDL: Lai, Baisheng, and Xiaojin Gong. “Saliency guided dictionary learning for weakly-supervised image parsing.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. Unsupervised Learning Features by Watching Objects Move: Pathak, Deepak, et al. “Learning Features by Watching Objects Move.” arXiv preprint arXiv:1612.06370 (2016). :star::star::star::star::star: SimGAN: Shrivastava, Ashish, et al. “Learning from simulated and unsupervised images through adversarial training.” arXiv preprint arXiv:1612.07828 (2016). :star::star::star: OPN: Lee, Hsin-Ying, et al. “Unsupervised Representation Learning by Sorting Sequences.” arXiv preprint arXiv:1708.01246 (2017). :star::star::star: Transitive Invariance for Self-supervised Visual Representation Learning: Wang, Xiaolong, et al. “Transitive Invariance for Self-supervised Visual Representation Learning” Proceedings of the IEEE International Conference on Computer Vision. 2017. :star::star::star: Saliency RFCN: Wang, Linzhao, et al. “Saliency detection with recurrent fully convolutional networks.” European Conference on Computer Vision. Springer International Publishing, 2016. :star::star::star::star: Employs low-level contrast features (color, intensity and oritentation) as saliency prior maps Recurrent structure to refine the coarse inference Pre-train using semantic segmentation data to both leverage strong supervision from multiple object categories and capture the intrisic representation of generic objects NLDF: Luo, Zhiming, et al. “Non-Local Deep Features for Salient Object Detection.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. :star::star::star: U-Net architecture (upsample feature) Combine global feature and pixel feature (local score + global score) Contrast feature IoU Boundary Loss by approximating the saliency map gradient. DSS: Hou, Qibin, et al. “Deeply supervised salient object detection with short connections.” arXiv preprint arXiv:1611.04849 (2016). :star::star::star::star: MSRNet: Li, Guanbin, et al. “Instance-Level Salient Object Segmentation.” arXiv preprint arXiv:1704.03604 (2017). :star::star::star::star: Instance-level subset optimization training details Amulet: Zhang, Pingping, et al. “Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection.” arXiv preprint arXiv:1708.02001 (2017). :star::star::star::star: Attention SRN: Zhu, Feng, et al. “Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification.” arXiv preprint arXiv:1702.05891 (2017). :star::star::star::star: Talk G-RMI: Google. (Object Detection) slides 1st ImageNet and COCO Visual Recognition Challenges Joint Workshop: 2015. link 2nd ImageNet and COCO Visual Recognition Challenges Joint Workshop: 2016. link]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n-2016winter]]></title>
    <url>%2F2017%2F09%2F21%2Fcs231n-2016winter%2F</url>
    <content type="text"><![CDATA[Assignment 1基于 Python 3 实现 1. L1_1_KNNL1_1_KNN 2. L1_2_SVML1_2_SVM 3. L1_3_SoftmaxL1_3_Softmax 4. L1_4_Two_layer_netL1_4_Two_layer_net 5. L1_5_FeaturesL1_5_Features Assignment 2]]></content>
      <tags>
        <tag>Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zen of Python]]></title>
    <url>%2F2017%2F09%2F13%2FZen-of-Python%2F</url>
    <content type="text"><![CDATA[Python 程序设计准则 The Zen of Python, by Tim PetersBeautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren’t special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one– and preferably only one –obvious way to do it.Although that way may not be obvious at first unless you’re Dutch.Now is better than never.Although never is often better than right now.If the implementation is hard to explain, it’s a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea – let’s do more of those!]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fruits]]></title>
    <url>%2F2017%2F09%2F12%2FFruits%2F</url>
    <content type="text"><![CDATA[General Fruits: watermeloncantaloupe 哈密瓜pineapplepeachgrape 葡萄dragon fruitpearapplebananablueberryblackberryred datecucumbertomatoorangelitchicherry]]></content>
      <tags>
        <tag>Fruits</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EnglishPod-18]]></title>
    <url>%2F2017%2F09%2F01%2FEnglishPod-18%2F</url>
    <content type="text"><![CDATA[vocabulary: Title:protest: an event where people gather together to show disapproval of something.break out: start suddenly.bailout: the act of saving a company from money problems.(救助)outrage: something that is morally wrong. 侮辱have the nerve: dare toplacard: sign people hold at protests, usually a piece of cardboard on a stick.rally: public meetingdemonstrationopponentproponent]]></content>
      <tags>
        <tag>English Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Artificial Neural Network]]></title>
    <url>%2F2017%2F08%2F24%2FArtificial-Neural-Network%2F</url>
    <content type="text"><![CDATA[[TOC] 一元二次方程求根公式 Forward Evolution Backward Propagation Evolution: Calculate: Update： Partial derivative]]></content>
      <tags>
        <tag>Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EnglishPod-17]]></title>
    <url>%2F2017%2F08%2F21%2FEnglishPod-17%2F</url>
    <content type="text"><![CDATA[vocabulary: it’s about timebridesmaid: female friends or relatives who helps the bride at a weddinggroom: a man who is about to be married.goomsmanushers: man who help guest find their seats.]]></content>
      <tags>
        <tag>English Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Links of Basic understand of CNN]]></title>
    <url>%2F2017%2F08%2F18%2FLinks-of-Basic-understand-of-CNN%2F</url>
    <content type="text"><![CDATA[Basic understand 神经网络介绍http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C技术向：一文读懂卷积神经网络CNNhttp://www.cnblogs.com/nsnow/p/4562308.html深度学习（卷积神经网络）一些问题总结http://blog.csdn.net/nan355655600/article/details/17690029卷积神经网络（CNN）http://ibillxia.github.io/blog/2013/04/06/Convolutional-Neural-Networks/Deep Learning模型之：CNN卷积神经网络（一）深度解析CNNhttp://www.cnblogs.com/nsnow/p/4562363.html数据挖掘系列（10）——卷积神经网络算法的一个实现(转)http://blog.sina.com.cn/s/blog_4ff49c7e0102vl5m.htmlMatlab/DeepLearnToolboxhttps://github.com/rasmusbergpalm/DeepLearnToolboxDeep Learning论文笔记之（四）CNN卷积神经网络推导和实现http://blog.csdn.net/zouxy09/article/details/9993371Deep Learning论文笔记之（五）CNN卷积神经网络代码理解http://blog.csdn.net/zouxy09/article/details/9993743斯坦福 池化http://ufldl.stanford.edu/wiki/index.php/%E6%B1%A0%E5%8C%96CNN神经网络层次分析http://blog.csdn.net/liulina603/article/details/44915905深度学习笔记1(卷积神经网络)http://blog.csdn.net/lu597203933/article/details/46575779CNN公式推导http://blog.csdn.net/lu597203933/article/details/46575871前向型神经网络之BPNN(附源码)http://blog.csdn.net/heyongluoyao8/article/details/48213345残差与误差的区别http://wenku.baidu.com/link?url=DUDkyV1tnD_SEGzgcxb9AaFU5VUcP9ISNR8q39-fpCcq_LGUHY7ucx5vDwr-MCfU_ofr7yIQZ_UgTfiivTtaDOulW2DD3pGs07eYmiQv5P7反向传导算法http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95图像卷积与滤波的一些知识点http://blog.csdn.net/zouxy09/article/details/49080029CNN卷积神经网络原理简介+代码详解http://doc.okbase.net/u012162613/archive/126058.html卷积神经网络（lenet）http://deeplearning.net/tutorial/lenet.html激活函数的作用https://www.zhihu.com/question/22334626神经网络入门第一部分http://blog.sina.com.cn/s/blog_6a67b5c50100tspb.html神经网络入门第二部分http://blog.sina.com.cn/s/blog_6a67b5c50100tspe.html卷积神经网络全面解析http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xiDeep learning：四十一(Dropout简单理解)http://www.cnblogs.com/tornadomeet/p/3258122.htmlDeepLearning (六) 学习笔记整理：神经网络以及卷积神经网络http://www.07net01.com/2015/11/963741.html深度卷积网络CNN与图像语义分割http://blog.csdn.net/xiahouzuoxin/article/details/47789361MATLAB conv2卷积的实现http://blog.csdn.net/celerychen2009/article/details/38852105]]></content>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EnglishPod-16]]></title>
    <url>%2F2017%2F08%2F18%2FEnglishPod-16%2F</url>
    <content type="text"><![CDATA[vocabulary: step on it = drive fast =speed uphaving a fit = really angry (having a fit is not going to help!)what’s the rush: jishenmecut through: go throughshort cutPhrase:make a left= turn leftmake a U turnare you nuts?(insane mad)]]></content>
      <tags>
        <tag>English Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EnglishPod-15]]></title>
    <url>%2F2017%2F08%2F17%2FEnglishPod-15%2F</url>
    <content type="text"><![CDATA[I’m sorry, I love you! vocabulary: explodeaccidentfamiliarplace: hoursecoincidenceknock you over ; knock over my coffee cupapologetic : feeling regretlove at first sight]]></content>
      <tags>
        <tag>English Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EnglishPod-14]]></title>
    <url>%2F2017%2F08%2F16%2FEnglishPod-14%2F</url>
    <content type="text"><![CDATA[vocabulary: recession: a period of time when the economy of a country is bad.debtmortgage: 抵押hit me pretty hardon top of all thattuitiona loan: amount of money borrowed.downturn: a time when the economy is worse than usual.]]></content>
      <tags>
        <tag>English Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Englishpod-13]]></title>
    <url>%2F2017%2F08%2F15%2FEnglishpod-13%2F</url>
    <content type="text"><![CDATA[vocabulary: expensesthrough the roofgo overprofit and lossoff the chartsthe list goes on and on]]></content>
      <tags>
        <tag>English Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[August Pictures]]></title>
    <url>%2F2017%2F08%2F15%2FAug-Pics%2F</url>
    <content type="text"><![CDATA[Non-Original Pictures(comes form https://dribbble.com/)]]></content>
      <tags>
        <tag>Pictures</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Bit Note of Markdown Rules]]></title>
    <url>%2F2017%2F08%2F14%2FA-Bit-Note-of-Markdown-Rules%2F</url>
    <content type="text"><![CDATA[概述 易读易写兼容HTML语法种类只对应HTML标记的一小部分 标题 123456#a##a###a####a#####a######a 引用 1&gt;aaaaaa 列表 123* aa* aaa* aaaa1231. Bird2. McHale1. Parish 分割线 * 图片 123![Alt text](/path/to/img.jpg)![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 链接 baidu 粗体和斜体 Markdown 的粗体和斜体也非常简单，用两个 * 包含一段文本就是粗体的语法，用一个 * 包含一段文本就是斜体的语法。例如： 粗体 斜体 代码框如果你是个程序猿，需要在文章里优雅的引用代码框，在 Markdown 下实现也非常简单，只需要用两个 ` 把中间的代码包裹起来，如 `code`。 符号**Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号：123456789101112\ 反斜线` 反引号* 星号_ 底线&#123;&#125; 花括号[] 方括号() 括弧# 井字号+ 加号- 减号. 英文句点! 惊叹号]]></content>
      <categories>
        <category>Edit</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Bigbang Theory]]></title>
    <url>%2F2017%2F08%2F14%2FThe-Bigbang-Theory%2F</url>
    <content type="text"><![CDATA[12'$ hexo clean $ hexo g -d' More info: Reading Our whole universe was in a hot dense state, 宇宙一度又烫又稠密Then nearly fourteen billion years ago expansion started. Wait…140亿年前终于爆了炸 等着瞧…The Earth began to cool,地球开始降温The autotrophs began to drool,自养生物来起哄Neanderthals developed tools,尼安德特人发明工具We built a wall (we built the pyramids),我们建长城（我们建金字塔）Math, science, history, unraveling the mysteries,数学 自然科学 历史 揭开神秘That all started with the big bang!一切由大爆炸开始“Since the dawn of man” is really not that long,其实人类历史没有多久As every galaxy was formed in less time than it takes to sing this song.星系形成时间比唱完这支歌还要短A fraction of a second and the elements were made.元素在微秒间便形成了The bipeds stood up straight,两足动物直立行走The dinosaurs all met their fate,恐龙都得认命了They tried to leap but they were late想要突变 没来得及And they all died (they froze their asses off)就死光光了（pp都冻成了化石） The oceans and pangea大洋和泛古陆See ya, wouldn’t wanna be ya拜拜 才不想学你Set in motion by the same big bang!都是爆炸惹的祸It all started with the big BANG!一切从大爆炸开始 It’s expanding ever outward but one day宇宙向外膨胀 但有一天It will cause the stars to go the other way,星球会反方向运动Collapsing ever inward, we won’t be here, it won’t be hurt向内坍塌 反正我们不在了 不会觉得疼Our best and brightest figure that it’ll make an even bigger bang!我们美好光辉的形象将引发一场更大的爆炸Australopithecus would really have been sick of us南方古猿肯定不爽我们Debating out while here they’re catching deer (we’re catching viruses)在他们捉鹿时唧唧歪歪 （我们现在捉电脑病毒了）Religion or astronomy, Encarta, Deuteronomy宗教 天文 e百科 旧约申命记It all started with the big bang!一切从大爆炸开始Music and mythology, Einstein and astrology音乐 神化 爱因斯坦 占星术It all started with the big bang!一切从大爆炸开始It all started with the big BANG!一切从大爆炸开始]]></content>
      <categories>
        <category>Lyric</category>
      </categories>
      <tags>
        <tag>loving lyric</tag>
      </tags>
  </entry>
</search>
